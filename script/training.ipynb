{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Weights and Biases eğitim verilerini kaydetmek için\n",
    "!pip install --upgrade wandb\n",
    "#wandb kütüphanesinin çağrılması\n",
    "#ve API için giriş\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerekli kütüphanelerin çağrılması\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#veriseti\n",
    "merge_df = pd.read_csv('../data/double_expression_dataframe.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verisetin tensör haline gelmesi için\n",
    "#önce \"array\" formatına dönüştürülmesi\n",
    "#daha sonra reshape edilmesi\n",
    "gene_df_values = preprocessing.normalize(merge_df.iloc[:,2:176].values)\n",
    "target_df_values = preprocessing.normalize(merge_df.iloc[:,176:350].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in range(len(gene_df_values)):\n",
    "    x_arr = [gene_df_values[i],target_df_values[i]]\n",
    "    lst.append(x_arr)\n",
    "X = np.asarray(lst)\n",
    "pair_info = merge_df[[\"gene\",\"target\",\"pair\"]]\n",
    "y = pair_info[\"pair\"].values\n",
    "X = X.reshape(18592,2,174,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OneHotEncoder ile etiketin uygun formata dönüştürülmesi\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder(sparse=False)\n",
    "y_reshape = y.reshape(len(y), 1)\n",
    "y = onehotencoder.fit_transform(y_reshape)\n",
    "#Data eğitim ve test diye ikiye ayırıldı\n",
    "#modelin görmediği datalar için nasıl sonuç\n",
    "#verdiği bu şekilde anlaşılabilir\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "print('Shape of x_train: ', X_train.shape)\n",
    "print('Shape of y_train: ', y_train.shape)\n",
    "print('Shape of x_test: ', X_test.shape)\n",
    "print('Shape of y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model yapısının kurulması\n",
    "def Model():\n",
    "  inputs = keras.layers.Input(shape=(2, 174,1))\n",
    "\n",
    "  x = keras.layers.SeparableConv2D(filters=32, kernel_size=(3,3), activation='relu',padding=\"same\")(inputs)\n",
    "  x = keras.layers.Dropout(.1)(x)\n",
    "  x = keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "  x = keras.layers.SeparableConv2D(filters=16, kernel_size=(3,3), activation='relu',padding=\"same\")(x)\n",
    "  x = keras.layers.Dropout(.1)(x)\n",
    "  x = keras.layers.BatchNormalization()(x)\n",
    "  x = keras.layers.Flatten()(x)\n",
    "  x = keras.layers.Dropout(.1)(x)\n",
    "  x = keras.layers.Dense(8,activation=\"relu\")(x)\n",
    "  outputs = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "  return keras.models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb'nin başlatılması\n",
    "run = wandb.init(project='tf-target-prediction-2',\n",
    "                 config={  # hiperparametre ve metadata'nın ayarlanması\n",
    "                     \"learning_rate\": 0.001,\n",
    "                     \"epochs\": 100,\n",
    "                     \"batch_size\":216,\n",
    "                     \"loss_function\": \"binary_crossentropy\",\n",
    "                     \"architecture\": \"CNN\"\n",
    "                 })\n",
    "config = wandb.config  # \n",
    "\n",
    "# Modeli başlatma\n",
    "tf.keras.backend.clear_session()\n",
    "model = Model()\n",
    "model.summary()\n",
    "\n",
    "# Modeli \"compile\" etme\n",
    "#optimizasyonu ayarlama\n",
    "optimizer = tf.keras.optimizers.RMSprop(config.learning_rate) \n",
    "model.compile(optimizer, config.loss_function, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We train with our beloved model.fit\n",
    "# Notice WandbCallback is used as a regular callback\n",
    "# We again use config\n",
    "history = model.fit(X_train, y_train,\n",
    "          epochs=config.epochs, \n",
    "          batch_size=config.batch_size,\n",
    "          validation_data=(X_test, y_test),\n",
    "          callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Error Rate: ', round((1 - accuracy) * 100, 2))\n",
    "\n",
    "# With wandb.log, we can easily pass in metrics as key-value pairs.\n",
    "wandb.log({'Test Error Rate': round((1 - accuracy) * 100, 2)})\n",
    "\n",
    "run.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn import preprocessing\n",
    "gedf = pd.read_csv(\"../data/genecoexpression.txt\")\n",
    "\n",
    "def rSubset(arr, r=2):\n",
    "    if len(arr) >= 2:\n",
    "        pair_lst = list(combinations(arr, 2))\n",
    "        return pair_lst\n",
    "    else:\n",
    "        print(\"Gene list have to be longer than 2 genes.\")\n",
    "        \n",
    "\n",
    "def find_expression(tf, target): \n",
    "    if (tf in gedf.gene.values) == True:\n",
    "        if (target in gedf.gene.values) == True:\n",
    "            e1 = gedf[gedf[\"gene\"] == \"{}\".format(tf)].iloc[:,1:].values\n",
    "            e2 = gedf[gedf[\"gene\"] == \"{}\".format(target)].iloc[:,1:].values\n",
    "            e1_n = preprocessing.normalize(e1)\n",
    "            e2_n = preprocessing.normalize(e2)\n",
    "            pair_expression = np.concatenate((e1_n,e2_n)).reshape(2,174,1)\n",
    "            return pair_expression, tf, target\n",
    "        else:\n",
    "            print(\"{} unkown expression level\".format(target))\n",
    "    else:\n",
    "        print(\"{} unkown expression level\".format(tf))\n",
    "            \n",
    "\n",
    "def find_expression_from_list(arr):\n",
    "    pair_list = rSubset(arr)\n",
    "    pair_list = np.char.upper(pair_list)\n",
    "    expression_array = []\n",
    "    tf_target_pair = []\n",
    "    for pair in pair_list:\n",
    "        tf = pair[0]\n",
    "        target = pair[1]\n",
    "        pair_expression, tf, target = find_expression(tf, target)\n",
    "        tf_target = np.array([tf,target])\n",
    "        expression_array.append((pair_expression))\n",
    "        tf_target_pair.append(tf_target)\n",
    "    return np.asarray(expression_array), tf_target_pair\n",
    "\n",
    "def make_prediction(genes):\n",
    "  xnew, tf_target_pair = find_expression_from_list(arr=genes)\n",
    "  pred = model.predict(xnew)\n",
    "  pred = onehotencoder.inverse_transform(pred)\n",
    "  for i in range(len(xnew)):\n",
    "    print(\"TF-Target Pair=%s, Predicted=%s\" % (tf_target_pair[i],pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction([\"HNF1B\",\"DPP4\",\"ACE2\",\"SPP1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
